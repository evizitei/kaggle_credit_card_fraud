---
title: "CreditCardFraud"
output: html_notebook
---

Trying to reproduce results from
and expert kaggler kernel:

https://www.kaggle.com/nschneider/gbm-vs-xgboost-vs-lightgbm

```{r}
library(pROC, quietly = TRUE)
library(microbenchmark, quietly = TRUE)

set.seed(42)

credit.card.data = read.csv("./data/creditcard.csv")
```

Data is loaded, now we'll split into train and
test data.

```{r}
train.test.split <- sample(2
  , nrow(credit.card.data)
  , replace = TRUE
  , prob = c(0.7, 0.3))
test = credit.card.data[train.test.split == 2, ]
```

```{r}
train = credit.card.data[train.test.split == 1, ]
```

So now we have a training set and a test set
randomly chosen at 70/30.

The author of the source notebook starts with
the gradient boosting technique via the GBM
package.  It's been a while since I've read over
gradient boosting, so re-summarizing here:

Train many "weak" models that are not good classifiers
on their own, but that have *some* significance
along different axes.  They don't have to be great
models, just have some valid signal (and ideally
not be overlapping in what signals they capture).

Specifically, train weak learners progressively
on specifically the examples that we have not 
classified correctly with prior weak learners.

This still requires some way to combine them.
You could just average the votes from the learners,
but this would not be very informative.  The learners
are weighted according to how well they do
on their respective passes.

That's about as deep as I have time to review
at the moment so let's implement:

```{r}
library(gbm, quietly=TRUE)
```
```{r}
gbm.model <- gbm(Class ~ .
                 , distribution = "bernoulli"
                 , data = rbind(train, test)
                 , n.trees = 500
                 , interaction.depth = 3
                 , n.minobsinnode = 100
                 , shrinkage = 0.01
                 , bag.fraction = 0.5
                 , train.fraction = nrow(train)/(nrow(train) + nrow(test)))

```

And let's chart the quality of the predictions:

```{r}
best.iter = gbm.perf(gbm.model, method = "test")
gbm.feature.imp = summary(gbm.model, n.trees = best.iter)
gbm.test = predict(gbm.model, newdata=test, n.trees=best.iter)
auc.gbm = roc(test$Class, gbm.test, plot=TRUE, col="red")
```